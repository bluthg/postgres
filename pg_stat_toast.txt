Collect statistics on TOAST usage
=================================

Why?
Currently the only way to gather information about TOASTed values is guessing:
* stawidth in pg_statistic > ~ 2k?
* compare pg_[total_]relation_size() of table and TOAST table
* compare size of TOAST table and sum of size of rows/columns (?)

It may be of interest how effective TOAST is used:
* how often has a column been considered for a) compression b) externalization?
* how often has compression failed?
* how often has compression succeeded?
* what was the average size before/after compression?
  * i.e., what was the average compression factor?

What?
Provide a view pg_stat_toast:
schemaname   text / nspid oid
relname      text / relid oid
attname      text / attnum int
ext_cnt      bigint
comp_cnt     bigint
comp_success bigint
comp_time    double
avgsize      double
avgcompsize  double

How?
* Statistics are only created for columns that are actually touched by toast_tuple_try_compression() and toast_tuple_externalize() (in backend/access/table/toast_helper.c).

	* Carefull here, a toast table is just another table, which can have it's
	  own access method. Which means that it does not necessarilly have to go
	  via the api described in toast_helper.c. That does not mean that it should
	  not be used as an entry point, but the API of the collector should be a
	  bit more generic. This is just a thought of course.
	* The statistics are not for toast _tables_, but columns. 
	  Assuming that any storage engine that offers toast will use the helpers,
	  they are in fact the right point to hook into.
	  They "see" each and every try for compression (also unsuccessful ones) and externalization.

* Currently, there's no statistics collector for columns/attributes, all column stats are done by ANALYZE.

	* The proposed view, is not meant to be used by the planner, correct? If so,
	  should there be an interface between ANALYZE and the new stats collector
	  for values to be updated?
	* I doubt the planner will make use of the information for the time being. 
	  I mean, it _could_ improve estimates about how much data needs to be read/written from/to disk and if 
	  additional CPU cycles will be needed for the compression/decompression. No idea if there's also consideration of that for varlen types...!
	  And it has nothing to do with ANALYZE. We're talking "live" statistics, similar to tracking of function calls etc.

* The closest is the function stats collector, seems to be a reasonable starting point (PgStat_Function*() in include/pgstat.h)

	* Cool. Now, should it be part of a table_entry? How should it look like?
	* I propose to start with something a bit straight forward, for example
	  count how many times an entry was inserted in a toast table. That might
	  not necessarilly end up being part of the final api, but it seems small
	  and confined enough to give some understanding about the integration
	  of the stats collector. Let us call this for the sake of the argument
	  pgstat_count_toast_insert(). I would assume that it would need two
	  arguments, the toast relation id and ofcourse the counter. Let us not
	  worry just yet with delete and truncate. Then we will need a function to
	  fetch the results, pg_fetch_toast_entry? 
	* You're thinking table level again, do you? ;-)
	  The way toasting works is essentially:
		while there are uncompressed attributes and the tuple is > 2k:
			try to compress the largest (yet untouched) attribute
		while there are externizable attributes and the tuple is > 2k:
			externalize largest (yet untouched) attribute
 	  So just looking at the toast _table_ doesn't tell you which attribute
	  was toasted, how effective the compression worked etc.

	  Try:
	  create table toasttest (one text, two bytea);
	  insert into toasttest (one) string_agg(i::text,'   ') from generate_series(0,1000) j(i);
	  insert into toasttest (one) string_agg(i::text,'   ') from generate_series(0,10) j(i);
	  insert into toasttest (two) select string_agg(i::text,'   ')::bytea from generate_series(0,1000) j(i);
	  insert into toasttest (two) select string_agg(i::text,'   ')::bytea from generate_series(0,10) j(i);
	  select ('pg_toast.pg_toast_'||'toasttest'::regclass::oid)::regclass AS mytoasttable \gset
	  select length(one),length(two) from toasttest;
	  select chunk_id,sum(length(chunk_data))  from :mytoasttable group by chunk_id;

	  The above should give us _two_ lines: 
	  attnum 1 / "one":
	    ext_cnt = 1, comp_cnt = 1, comp_success = 1,
 	    comp_time = 0.0002 (ms, or so), avgsize = 5894, avgcompsize = 3190
	  attnum 2 / "two":
	    ext_cnt = 1, comp_cnt = 1, comp_success = 1,
 	    comp_time = 0.0002 (ms, or so), avgsize = 5894, avgcompsize = 3190

	  And that is much more detailed than the rough guesses we can make by
just looking at chunks etc.

	  I think adding some debug code in the two helper functions and writing 
	  some tests will enable us to see if the information we're looking for
	  will eventually come out of it.
